{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPGN268 - Geophysical Data Analysis\n",
    "## Types of Seismic Data – Part I: File manipulation\n",
    "#### Due: April 6, in class\n",
    "\n",
    "For the first part of this Data Story you will look at Seismic Data measured by [Distributed Acoustic Sensing](https://en.wikipedia.org/wiki/Distributed_acoustic_sensing)\n",
    "(DAS) cables deployed at Kafadar Commons as part of the [Geophysical Discovery Lab](https://people.mines.edu/rkrahenb/geophysical-discovery-lab/). The data was collected and pre-processes by [Dr. Eileen Martin](https://geophysics.mines.edu/project/martin-eileen/) and her Mathematical Geophysics students.\n",
    "\n",
    "- The data was collected on two separate days: Feb 17, 2023 and March 1, 2023.\n",
    "- The file names are tagged with the date and time of collection in UTC\n",
    "- The data was recorded at a rate of 500 samples per second\n",
    "- The measurements recorded correspond to strain and timing of the measurement\n",
    "\n",
    "\n",
    "You may discuss this assignment with your peers, but everyone should submit their assignment individually. If there is anyone who has *signficantly* contrbuted to your work, helped you figure something important out, etc., list them as a collaborator below with a short description of their input. **Collaborating will not impact your grade**. Please be honest.\n",
    "\n",
    "\n",
    "\n",
    "### Preparation\n",
    "\n",
    "- Navigate to the GPGN268-CORE directory and do a `git pull` to get this notebook. \n",
    "\n",
    "- Then navigate to your `ds03-seismology`, create a directory called `notebooks` and copy this notebook to your notebooks directory. \n",
    "\n",
    "```\n",
    "$ cd ~/work/classes/GPGN268/coursework-lastname/ds03-seismology/\n",
    "$ cp ~/work/classes/GPGN268/GPGN268-CORE/assignments/DS03-DAS.ipynb notebooks\n",
    "```\n",
    "\n",
    "- On Canvas, download the data files from files/data/DAS. There should be 11 files. \n",
    "Then create a sub-directory `data` in your `ds03-seismology` directory and move the `*.h5` files that you just downloaded there. When you are done, \n",
    "you should have somthing like this:\n",
    "\n",
    "```bash\n",
    "$ pwd\n",
    "~/work/classes/GPGN268/coursework-villasboas/ds03-seismology\n",
    "$ ls data\n",
    "Global_DAS_1078DT_25PR_14GL_5DEC_2023-02-17T164133Z.h5\n",
    "Global_DAS_1078DT_25PR_14GL_5DEC_2023-02-17T164233Z.h5\n",
    "Global_DAS_1078DT_25PR_14GL_5DEC_2023-02-17T164333Z.h5\n",
    "Global_DAS_1078DT_25PR_14GL_5DEC_2023-02-17T164433Z.h5\n",
    "Global_DAS_1078DT_25PR_14GL_5DEC_2023-02-17T164533Z.h5\n",
    "Global_DAS_1078DT_25PR_14GL_5DEC_2023-02-17T164633Z.h5\n",
    "Global_DAS_1078DT_25PR_14GL_5DEC_2023-03-01T231427Z.h5\n",
    "Global_DAS_1078DT_25PR_14GL_5DEC_2023-03-01T231527Z.h5\n",
    "Global_DAS_1078DT_25PR_14GL_5DEC_2023-03-01T231627Z.h5\n",
    "Global_DAS_1078DT_25PR_14GL_5DEC_2023-03-01T231727Z.h5\n",
    "Global_DAS_1078DT_25PR_14GL_5DEC_2023-03-01T231827Z.h5\n",
    "```\n",
    "\n",
    "- From the root of your `ds03-seismology` directory, launch Jupyter Lab. Remember to activate the GPGN268 conda environment first.\n",
    "\n",
    "```\n",
    "$ conda activate GPGN268\n",
    "$ jupyter lab\n",
    "```\n",
    "\n",
    "- Using the left navigation toolbar in Jupyter Lab, go to the `notebooks` directory rename this notebook to `dev.ipynb`– this will be where you will develop the code for your data story (try things out, make draft figures, etc). **You will not** turn in (i.e., push to GitHub) the `dev.ipynb` file. \n",
    "\n",
    "- Create another notebook called `ds03-seismology.ipynb`. This is where you will put the final version of your Data Story, with polished text, and clean and well-documented code.\n",
    "\n",
    "- Copy the text below onto the first cell (Markdown) of your `ds03-seismology.ipynb` notebook and fill it out with your name and date.\n",
    "\n",
    "```markdown\n",
    "# GPGN268 - Geophysical Data Analysis\n",
    "## Data Story 03 - Seismology\n",
    "\n",
    "**Student:** Blaster the Burro \n",
    "**Collaborators:**\n",
    "- Yoda helped me figure out how to use the force\n",
    "- Obi-Wan provided input on my code to plot resistivity\n",
    "**Date:** May the 4th, 2078\n",
    "```\n",
    "\n",
    "- Complete the tasks below. Use this notebook (`dev.ipynb`) to explore and follow the instructions. After your are done with the final version of your assignment, git add `ds03-seismology.ipynb`, commit, and push to GitHub."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading HDF5 data\n",
    "The DAS data that we will use is in HDF5 format. To read these data we will use a python library called [h5py](https://docs.h5py.org/en/stable/#)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "40yIC4bJbQar",
    "outputId": "6c004d41-0fa4-4cd5-e38f-b1ae1fcb333b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will list all files from 2023-02-17 using `glob` and wildcards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rS3D9wN_cFNN",
    "outputId": "c01a698d-26fb-4d07-c32a-45dece351d7f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../data/Global_DAS_1078DT_25PR_14GL_5DEC_2023-02-17T164133Z.h5',\n",
       " '../data/Global_DAS_1078DT_25PR_14GL_5DEC_2023-02-17T164233Z.h5',\n",
       " '../data/Global_DAS_1078DT_25PR_14GL_5DEC_2023-02-17T164333Z.h5',\n",
       " '../data/Global_DAS_1078DT_25PR_14GL_5DEC_2023-02-17T164433Z.h5',\n",
       " '../data/Global_DAS_1078DT_25PR_14GL_5DEC_2023-02-17T164533Z.h5',\n",
       " '../data/Global_DAS_1078DT_25PR_14GL_5DEC_2023-02-17T164633Z.h5']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_list = sorted(glob.glob('../data/Global_DAS_1078DT_25PR_14GL_5DEC_2023-02*.h5'))\n",
    "files_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read a file with `h5py` we use the module `h5py.File` and pass the name of the file followed by the argument `'r'`, which stands for \"reading mode\". Let's read the first file from our list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 file \"Global_DAS_1078DT_25PR_14GL_5DEC_2023-02-17T164133Z.h5\" (mode r)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = h5py.File(files_list[0], 'r')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have no idea what to do with this object, so let's use `dir` to try to find something about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_MutableMapping__marker',\n",
       " '__abstractmethods__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__class_getitem__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getnewargs__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__slots__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_d',\n",
       " '_e',\n",
       " '_gcpl_crt_order',\n",
       " '_id',\n",
       " '_ipython_key_completions_',\n",
       " '_lapl',\n",
       " '_lcpl',\n",
       " '_libver',\n",
       " 'attrs',\n",
       " 'build_virtual_dataset',\n",
       " 'clear',\n",
       " 'close',\n",
       " 'copy',\n",
       " 'create_dataset',\n",
       " 'create_dataset_like',\n",
       " 'create_group',\n",
       " 'create_virtual_dataset',\n",
       " 'driver',\n",
       " 'file',\n",
       " 'filename',\n",
       " 'flush',\n",
       " 'get',\n",
       " 'id',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'libver',\n",
       " 'meta_block_size',\n",
       " 'mode',\n",
       " 'move',\n",
       " 'name',\n",
       " 'parent',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'ref',\n",
       " 'regionref',\n",
       " 'require_dataset',\n",
       " 'require_group',\n",
       " 'setdefault',\n",
       " 'swmr_mode',\n",
       " 'update',\n",
       " 'userblock_size',\n",
       " 'values',\n",
       " 'visit',\n",
       " 'visititems']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object has the attribute `keys`. So let's try that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['Acquisition']>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and keep exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/Acquisition\" (2 members)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Acquisition']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['Custom', 'Raw[0]']>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Acquisition'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/Acquisition/Raw[0]\" (3 members)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Acquisition']['Raw[0]']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['Custom', 'RawData', 'RawDataTime']>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Acquisition']['Raw[0]'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 13963817,  15360819,  15728914, ...,  54194224, -72606198,\n",
       "        -75493199],\n",
       "       [ 13963223,  15360234,  15728365, ...,  54195131, -72605440,\n",
       "        -75492494],\n",
       "       [ 13962343,  15359339,  15727367, ...,  54193676, -72606535,\n",
       "        -75492884],\n",
       "       ...,\n",
       "       [ 14339230,  15755428,  16149981, ...,  54189850, -72609147,\n",
       "        -75494221],\n",
       "       [ 14339863,  15756100,  16150714, ...,  54190729, -72608397,\n",
       "        -75494259],\n",
       "       [ 14339059,  15755116,  16149678, ...,  54189972, -72608558,\n",
       "        -75493907]], dtype=int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Acquisition']['Raw[0]']['RawData'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bingo! It looks like we found where in this data structure the strain measurements are stored. It looks like there is some time information too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1676652093654000, 1676652093656000, 1676652093658000, ...,\n",
       "       1676652153648000, 1676652153650000, 1676652153652000])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Acquisition']['Raw[0]']['RawDataTime'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "## Task 1 – Reading the data\n",
    "\n",
    "Write a function that takes as input the path to a given DAS `.h5` file and returns the strain and timing\n",
    "\n",
    "```python\n",
    "def your_function(path_to_file):\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "    \n",
    "    return strain, timing\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating file names and paths\n",
    "\n",
    "Python has an incredibly useful library for manipulating files and paths called [os](https://docs.python.org/3/library/os.html). We've seen in the introduction that the time of the start of the data collection is written in the file name. We will use `os` to extract the time information from the filename.\n",
    "\n",
    "We know from using `dir` above that our `h5py` object has an attribute `filename`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 file \"Global_DAS_1078DT_25PR_14GL_5DEC_2023-02-17T164133Z.h5\" (mode r)>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/Global_DAS_1078DT_25PR_14GL_5DEC_2023-02-17T164133Z.h5'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`data.filename` has two parts: the path and the actual filename (also known as basename). We can extract the basename using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Global_DAS_1078DT_25PR_14GL_5DEC_2023-02-17T164133Z.h5'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(data.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we would like to grab just the portion that has `2023-02-17T164133Z`. We see that parts of the filename are separated by an underscore. We can use the function `split` to split the filename every time there is an underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Global', 'DAS', '1078DT', '25PR', '14GL', '5DEC', '2023-02-17T164133Z.h5']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(data.filename).split('_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's getting better. Now, we just want the last element of this list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-02-17T164133Z.h5'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(data.filename).split('_')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And split again where there is a `.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2023-02-17T164133Z', 'h5']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(data.filename).split('_')[-1].split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now, we want the first element of the list above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-02-17T164133Z'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(data.filename).split('_')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we probably don't want to include the \"Z\" in the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-02-17T164133'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(data.filename).split('_')[-1].split('.')[0][:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining all steps into a single line and saving it to a variable, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2023-02-17T164133'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_string = os.path.basename(data.filename).split('_')[-1].split('.')[0][:-1]\n",
    "time_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------\n",
    "## Task 2 – Time and time again\n",
    "\n",
    "### Task 2.1\n",
    "\n",
    "- Test the code above for other files in your `file_list` and see if the time extracted from the filename makes sense.\n",
    "\n",
    "- Using the steps above, write a function that takes as input the path to a given DAS `.h5` file and returns a string with the date and time portion of the filename.\n",
    "\n",
    "```python\n",
    "def your_function(path_to_file):\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "    \n",
    "    return time_string\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a time vector\n",
    "\n",
    "Now, the step above gave us a string with the time and date. We would really like to have an actual time with the proper units. The function `to_datetime` from `pandas` can help with that. For example, let's say I'm working with a time string `time_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('1989-05-21 03:14:00')"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_test = '1989-05-21 03:14' # This string has spaces and special characteres\n",
    "pd.to_datetime(time_test, format='%Y-%m-%d %H:%M') # format will tell pandas how to read the date and time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can be even more specific and pass to `pd.to_datetime` the argument `utc=True` to indicate that our timestamp is in reference to UTC time.\n",
    "\n",
    "### Task 2.2\n",
    "\n",
    "Now, try a similar approach to convert the time from your filename (`file_time`) to a timestamp:\n",
    "\n",
    "```python\n",
    "time_stamp = pd.to_datetime(....,format=...., utc=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-02-17 16:41:33+0000', tz='UTC')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_stamp = pd.to_datetime(time_string, format='%Y-%m-%dT%H%M%S', utc=True)\n",
    "time_stamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.3\n",
    "Given that these measurements were collected at a rate of 500 measurements per second, calculate:\n",
    "- The total duration of your record in seconds\n",
    "- The interval between two conssecutive measurements in seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.4\n",
    "\n",
    "`pandas` has a function `Timedelta` which allows adding a certain amount of time to a timestamp. For example, if you wanted to add 10 seconds to your `time_stamp`, you would do:\n",
    "\n",
    "```python\n",
    "time_stamp + pd.Timedelta(10, 's') # Value to add, units of the value (seconds in this case)\n",
    "```\n",
    "\n",
    "Using the method above and the values that you calculated in Task 2.3, create a variable `end_time` to save the timestamp for the last measurement in your dataset.\n",
    "\n",
    "```python\n",
    "end_time = time_stamp + pd.Timedelta(..., ...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.5\n",
    "\n",
    "Now you should have the start time (`time_stamp`), the end time (`end_time`) and you should know how many points you have in your data. This should be sufficient for you to create a time vector with the datetimes of all your records. To do that, use the functionn `date_range` from `pandas`, which is similar to the `numpy.range`. The first argument is the start date, the second argument is the end date, and the argument `periods` is the number of elements that you want in yout range (or the length of your record).\n",
    "\n",
    "```python\n",
    "time_utc = pd.date_range(start=..., end=..., periods=...)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.6\n",
    "The final step is to convert the time to Mountain time. You can achieve this by doing\n",
    "\n",
    "```python\n",
    "time = time_utc.tz_convert('US/Mountain')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.7\n",
    "\n",
    "Combine all the steps that you used for creating the time into a function that takes as input the input the path to a given DAS .h5 file and returns a datetime vector referenced to US/Mountain time.\n",
    "\n",
    "\n",
    "```python\n",
    "def your_function(path_to_file):\n",
    "    ...\n",
    "    ...\n",
    "    \n",
    "    return time\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "## Task 3\n",
    "\n",
    "The data in the .h5 files is saved as strain, but for geophysics applications we're more interested in the **strain rate**. Write a function that takes as input the strain and a datetime vector and returns the strain **rate**:\n",
    "\n",
    "$$\n",
    "Strain\\ Rate = \\frac{Strain}{\\Delta t}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------\n",
    "## Task 4\n",
    "\n",
    "- Clean up your functions.\n",
    "- Combining the steps and functions that we did in the previous tasks, write a function that takes as input the path to a given DAS .h5 file and returns the strain rate and the time of each measurement as a datetime object converted to MST. \n",
    "- Document your functions with a `docstring` and use expressive variable names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
